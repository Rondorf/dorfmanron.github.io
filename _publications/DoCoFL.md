---
title: "DoCoFL: Downlink Compression for Cross-Device Federated Learning"
collection: publications
permalink: /publications/DoCoFL
excerpt: "Many compression techniques have been proposed to reduce the communication overhead of Federated Learning training procedures. However, these are typically designed for compressing model updates, which are expected to decay throughout training. As a result, such methods are inapplicable to downlink (i.e., from the parameter server to clients) compression in the cross-device setting, where heterogeneous clients <i>may appear only once</i> during training and thus must download the model parameters. Accordingly, we propose DoCoFL -- a new framework for downlink compression in the cross-device setting. Importantly, DoCoFL can be seamlessly combined with many uplink compression schemes, rendering it suitable for bi-directional compression. Through extensive evaluation, we show that DoCoFL offers significant bi-directional bandwidth reduction while achieving competitive accuracy to that of a baseline without any compression.  <br> <br> <img src='/images/DoCoFL.png'> <br> <br> <a href='https://openreview.net/pdf?id=VxKr51JjWC'>PDF</a> <br>"
venue: '40th International Conference on Machine Learning (ICML)'
date: 2023-04-24
pdfurl: 'https://openreview.net/pdf?id=VxKr51JjWC'
---  
We consider stochastic optimization problems where data is drawn from a Markov chain. Existing methods for this setting crucially rely on knowing the mixing time of the chain, which in real-world applications is usually unknown. We propose the first optimization method that does not require the knowledge of the mixing time, yet obtains the optimal asymptotic convergence rate when applied to convex problems. We further show that our approach can be extended to: (i) finding stationary points in non-convex optimization with Markovian data, and (ii) obtaining better dependence on the mixing time in temporal difference (TD) learning; in both cases, our method is completely oblivious to the mixing time. Our method relies on a novel combination of multi-level Monte Carlo (MLMC) gradient estimation together with an adaptive learning method. <br> <br>
<a href='https://openreview.net/pdf?id=VxKr51JjWC'>PDF</a> <br> <br> 
<img src='/images/DoCoFL.png'>